## 1. Эксперименты с глубиной сети
Исследовались полносвязные нейронные сети с разной глубиной:
- 1 слой (линейный классификатор)
- 2 слоя (1 скрытый)
- 3 слоя (2 скрытых)
- 5 слоёв (4 скрытых)
- 7 слоёв (6 скрытых)

Параметры эксперимента:
- Датасеты: MNIST и CIFAR-10
- Режимы: с регуляризацией (Dropout + BatchNorm) и без
- Количество эпох: 15
- Batch size: 128
- Оптимизатор: Adam

## Результат:

### MNIST (без регуляризации)
| Глубина | Train Acc | Test Acc | Gap  |
|---------|-----------|----------|------|
| 1       | 92.99%    | 92.50%   | 0.49%|
| 2       | 99.65%    | 98.12%   | 1.53%|
| 3       | 99.56%    | 97.81%   | 1.75%|
| 5       | 99.49%    | 97.90%   | 1.59%|
| 7       | 99.39%    | 97.84%   | 1.55%|

**Оптимальная глубина**: 1 слой (наименьший gap)

### CIFAR-10 (без регуляризации)
| Глубина | Train Acc | Test Acc | Gap  |
|---------|-----------|----------|------|
| 1       | 39.98%    | 33.92%   | 6.06%|
| 2       | 70.79%    | 51.21%   | 19.58%|
| 3       | 78.13%    | 52.80%   | 25.33%|
| 5       | 79.55%    | 53.83%   | 25.72%|
| 7       | 77.55%    | 53.03%   | 24.52%|

**Оптимальная глубина**: 1 слой (наименьший gap)

### MNIST (с регуляризацией)
| Глубина | Train Acc | Test Acc | Gap  | Time(s) |
|---------|-----------|----------|------|---------|
| 1       | 93.06%    | 92.30%   | 0.76%| 78.91   |
| 2       | 99.27%    | 98.02%   | 1.25%| 89.36   |
| 3       | 99.18%    | 98.29%   | 0.89%| 88.15   |
| 5       | 98.81%    | 98.44%   | 0.37%| 96.74   |
| 7       | 98.50%    | 98.41%   | 0.09%| 104.66  |

- Test accuracy выросла на 0.5-1% для глубоких сетей
- Gap уменьшился в 2-17 раз для сетей глубиной 3+ слоя

**Оптимальная глубина**: 7 слой

### CIFAR-10 (с регуляризацией)
| Глубина | Train Acc | Test Acc | Gap  | Time(s) |
|---------|-----------|----------|------|---------|
| 1       | 39.97%    | 35.22%   | 4.75%| 83.62   |
| 2       | 61.99%    | 54.82%   | 7.17%| 93.90   |
| 3       | 63.90%    | 56.53%   | 7.37%| 105.39  |
| 5       | 61.93%    | 56.70%   | 5.23%| 108.00  |
| 7       | 60.19%    | 56.03%   | 4.16%| 122.31  |

- Test accuracy выросла на 2-4% для всех конфигураций
- Gap уменьшился в 1.5-6 раз

**Оптимальная глубина**: 7 слой


## Вывод:

- Регуляризация для MNIST: позволила использовать более глубокие сети (оптимум с 1 на 7 слоёв)
- Регуляризация для CIFAR: сократила переобучение, но результаты все еще не идеальны
- Без регуляризации на графиках обоих датасетов видно переобучение при использовании более 1 слоя
- Время обучения увеличивается в зависимости от глубины нейронной сети

## 2. Эксперименты с шириной сети

### Результаты для MNIST

| Конфигурация | Ширина слоев      | Параметры | Время (с) | Train Acc | Test Acc |
|--------------|-------------------|-----------|-----------|-----------|----------|
| Узкая        | [64, 32, 16]      | 53,018    | 80.06     | 98.80%    | 97.35%   |
| Средняя      | [256, 128, 64]    | 242,762   | 80.54     | 99.55%    | 97.99%   |
| Широкая      | [1024, 512, 256]  | 1,462,538 | 108.69    | 99.63%    | 98.16%   |
| Очень широкая| [2048, 1024, 512] | 4,235,786 | 110.32    | 99.52%    | 98.40%   |

### Результаты для CIFAR

| Конфигурация | Ширина слоев      | Параметры | Время (с) | Train Acc | Test Acc |
|--------------|-------------------|-----------|-----------|-----------|----------|
| Узкая        | [64, 32, 16]      | 199,450   | 92.07     | 61.06%    | 50.64%   |
| Средняя      | [256, 128, 64]    | 828,490   | 124.40    | 76.95%    | 53.10%   |
| Широкая      | [1024, 512, 256]  | 3,805,450 | 103.94    | 84.34%    | 53.01%   |
| Очень широкая| [2048, 1024, 512] | 8,921,610 | 118.07    | 83.13%    | 54.12%   |

**Выводы:**
1. Для MNIST все конфигурации показывают хорошие результаты (>97% accuracy)
2. Увеличение ширины дает небольшой прирост точности на MNIST
3. На CIFAR результаты значительно хуже (~50-54% accuracy)
4. Слишком широкие сети на CIFAR склонны к переобучению (разрыв между train и test accuracy)
5. Время обучения не линейно растет с увеличением ширины

## Оптимизация архитектуры (Grid Search)

### Лучшие результаты для MNIST

1. `[512, 256, 64]` - 98.30% accuracy (550,346 параметров)
2. `[512, 128, 128]` - 98.20% accuracy (485,386 параметров)
3. `[256, 128, 128]` - 98.04% accuracy (251,658 параметров)

### Лучшие результаты для CIFAR

1. `[512, 128, 128]` - 54.24% accuracy (1,656,842 параметров)
2. `[512, 128, 64]` - 54.18% accuracy (1,647,946 параметров)
3. `[256, 256, 32]` - 53.99% accuracy (861,034 параметров)

**Тепловые карты:**
1. Для MNIST лучшие результаты достигаются при средних/больших размерах всех слоев
2. Для CIFAR оптимальны конфигурации с большим первым слоем (512) и средними последующими
3. На CIFAR наблюдается слабая зависимость точности от архитектуры

## Вывод

1. Для MNIST достаточно узких/средних сетей (256-512 нейронов)
2. Для CIFAR требуется более широкая сеть, но точность остается низкой
3. Возможно, для CIFAR нужно:
   - Увеличить глубину сети
   - Добавить регуляризацию (Dropout, BatchNorm)
   - Увеличить количество эпох обучения
4. Оптимальные схемы изменения ширины:
   - Для MNIST: постоянная или слабо убывающая
   - Для CIFAR: резко убывающая (512 - 128 - 64)

## Временные характеристики

1. Обучение на MNIST быстрее (~50-110с на конфигурацию)
2. CIFAR требует больше времени (~60-120с на конфигурацию)
3. Время растет с увеличением количества параметров

## Итог

Ширина сети важна, но не является единственным фактором. Для сложных датасетов требуется более тщательный подбор архитектуры и методов регуляризации.

## 3. Эксперименты с регуляризацией

### Сравнение методов регуляризации

1. Без регуляризации

- точность: 97.98%
- стабильное обучение, рост точности с 95.31% до 97.98%
- веса распределены свободно

2. Dropout (p=0.1)

- точность: 97.49%
- медленнее достигает максимума
- умеренное ограничение весов

3. Dropout (p=0.3)

- точность: 97.70%
- плавный рост точности
- сильнее ограничивает веса в последних слоях

4. Dropout (p=0.5)

- точность: 97.63%
- наибольшая стабильность среди вариантов Dropout
- сильное ограничение весов во всех слоях

5. Только BatchNorm

- точность: 98.06%
- быстрый рост в начале, затем стабилизация
- нормализует распределения активаций по слоям

6. Dropout+BN

- точность: 97.78%
- плавный устойчивый рост

7. L2 регуляризация
- точность: 98.03%
- колебания в процессе обучения
- равномерное ограничение весов по всем слоям

### Адаптивные методы регуляризации

1. Адаптивный Dropout

- точность: 98.12%
- стабильное обучение
- постепенное увеличение регуляризации улучшает обобщение

2. BatchNorm (momentum=0.1)

- точность: 97.56%
- стабильность немного хуже стандартного BN
- более адаптивная нормализация по слоям

3. Комбинированная

- точность: 97.49%
- сбалансированный эффект на разных слоях

4. Постепенная регуляризация
- точность: 97.89%
- разная степень регуляризации по слоям

## Вывод

Наивысшую точность показали:

- Адаптивный Dropout (98.12%)

- BatchNorm (98.06%)

- L2 регуляризация (98.03%)

BatchNorm - лучшая стабильность обучения

Dropout с p=0.3-0.5 обеспечивает баланс между точностью и стабильностью

Постепенная регуляризация по слоям дает хорошие результаты, подтверждая важность дифференцированного подхода
