## 1.1 Расширение линейной регрессии

**Реализовано:**
- Добавлены L1 и L2 регуляризации через метод `regularization_loss()`
- Реализован early stopping при отсутствии улучшения за 5 эпох

**Результаты:**
- Обучение остановилось на 34 эпохе (лучший loss = 0.0818)
- График потерь сохранён в `plots/linreg_loss.png`
- Модель сохранена в `models/linreg.pth`

## 1.2 Расширение логистической регрессии

**Реализовано:**
- Поддержка многоклассовой классификации (3 класса в тестовых данных)
- Метрики качества:
  - Precision: 0.911
  - Recall: 0.914 
  - F1-score: 0.910
- Визуализация:
  - Графики потерь и точности (`plots/logreg_training.png`)
  - Confusion matrix (`plots/confusion_matrix.png`)
  - Модель сохранена в `models/logreg.pth`


## 2.1 Кастомный Dataset класс

**Реализовано в классе `CustomCSVDataset`:**
- Загрузка данных из CSV файла с автоматическим определением типов признаков
- Комплексная предобработка данных:
  - Удаление выбросов по правилу IQR для числовых признаков
  - Кодирование категориальных признаков (LabelEncoder)
  - Нормализация числовых признаков (StandardScaler)
  - Автоматическое определение типа задачи (регрессия/классификация)
  - Обработка целевой переменной (нормализация/кодирование)
- Поддержка различных форматов данных:
  - Числовые (нормализация)
  - Категориальные (кодирование)
  - Бинарные


## 2.2 Эксперименты с различными датасетами

### Линейная регрессия (house_sales.csv)
**Результаты:**
- Остановка на 15 эпохе (лучший loss = 0.0321)
- Метрики на тестовой выборке:
  - MSE: 0.0101
  - MAE: 0.0660
  - R²: 0.9898
- График обучения: `plots/house_sales_regression_loss.png`
- Сохранённая модель: `models/house_sales_regression.pth`

### Логистическая регрессия (play_tennis_dataset.csv)
**Результаты:**
- Обучение завершено за 100 эпох
- Финальные метрики:
  - Accuracy: ~71.5%
  - Precision: 0.736
  - Recall: 0.837
  - F1: 0.783
- Визуализации:
  - Графики обучения: `plots/tennis_classification_training.png`
  - Confusion matrix: `plots/tennis_confusion_matrix.png`
- Сохранённая модель: `models/classification.pth`


## 3.1 Исследование гиперпараметров

### Регрессия (house_sales.csv)
**Исследованные комбинации:**
- Learning rates: [0.01, 0.1]
- Batch sizes: [32, 64]
- Оптимизаторы: SGD, Adam

**Анализ графиков:**
1. На графике `regression_mse_comparison.png`:
По оси Y показаны значения mse, по оси X - номера экспериментов 0-7, где каждый эксперимент - уникальная комбинация Learning rate (0.01 или 0.1), Batch size (32 или 64), Оптимизатора (SGD или Adam).
Видно, что наименьший MSE был достигнут с Learning rate=0.01 и Batch size=32 оптимизатором Adam.

2. На графике `regression_r2_comparison.png`:
По оси Y показаны значения r2, по оси X - номера экспериментов 0-7, где каждый эксперимент - уникальная комбинация Learning rate (0.01 или 0.1), Batch size (32 или 64), Оптимизатора (SGD или Adam).
SGD (оранжевый) стабильно показывает R² ~0.988-0.990.
Максимальное r2=0.990 достигнуто с SGD (при lr=0.1 и batch_size=64, судя по положению точки).
Линия SGD почти горизонтальна, что говорит о слабой зависимости от других гиперпараметров.

3. На графике `regression_mae_comparison.png`:
По оси Y показаны значения MAE, по оси X - номера экспериментов 0-7, где каждый эксперимент - уникальная комбинация Learning rate (0.01 или 0.1), Batch size (32 или 64), Оптимизатора (SGD или Adam).
Видно, что оптимизатор SGD дает более стабильные и низкие показатели при разных гиперпараметрах.


### Классификация (play_tennis_dataset.csv)
**Ключевые наблюдения:**
1. График `classification_accuracy_comparison.png`:
По оси Y показаны значения accuracy, по оси X - номера экспериментов 0-7, где каждый эксперимент - уникальная комбинация Learning rate (0.01 или 0.1), Batch size (32 или 64), Оптимизатора (SGD или Adam).
Оптимизатор SGD дает более стабильные и высокие показатели при разных гиперпараметрах, но лучше всего accuracy получился при lr=0.1, batch_size=32.

2. График `classification_f1_comparison.png`:
оY - значения F1, оX - номера экспериментов 0-7,  каждый эксперимент - уникальная комбинация Learning rate (0.01 или 0.1), Batch size (32 или 64), Оптимизатора (SGD или Adam).
SGD дает более стабильные и высокие показатели при разных гиперпараметрах, лучше всего F1 при lr=0.1, batch_size=32.

3. График `classification_recall_comparison.png`:
оY - значения recall, оX - номера экспериментов 0-7,  каждый эксперимент - уникальная комбинация Learning rate (0.01 или 0.1), Batch size (32 или 64), Оптимизатора (SGD или Adam).
SGD дает более стабильные показатели при разных гиперпараметрах, возможно, гиперпараметры совсем не влияют на эту метрику, или данные слишком простые, или в эксперименте допущена ошибка.

4. График `classification_precision_comparison.png`:
оY - значения precision, оX - номера экспериментов 0-7,  каждый эксперимент - уникальная комбинация Learning rate (0.01 или 0.1), Batch size (32 или 64), Оптимизатора (SGD или Adam).
SGD дает более высокие показатели при lr=0.1, batch_size=32 (4 эксперимент).

## 3.2 Feature Engineering

### Регрессионная модель
**Сравнение на графике `regression_feature_engineering_comparison.png`:**
MSE уменьшился, но немного уменьшился и r2 из-за добавления шумных полиномиальных признаков, а MAE вырос, т.к. модель стала более чувствительной к выбросам.

### Классификационная модель
**Сравнение на графике `classification_feature_engineering_comparison.png`:**
Все метрики увеличились после добавления новых признаков.
